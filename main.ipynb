{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main-rgb.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjVtyFndzYlq",
        "outputId": "98f3bb4f-d7b2-4cc6-e0b5-08e6e598a4c8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVit-q8_zZO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58e620ca-988d-4b93-cb6a-cfdf14aa682e"
      },
      "source": [
        "!pip install -q transformers flax\n",
        "!pip install -q git+https://github.com/patil-suraj/vqgan-jax.git  # VQGAN model in JAX\n",
        "!pip install -q git+https://github.com/borisdayma/dalle-mini.git  # Model files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.1 MB 4.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 207 kB 33.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 43.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 36.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 41.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 126 kB 47.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.5 MB/s \n",
            "\u001b[?25h  Building wheel for vqgan-jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 235 kB 4.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.5 MB/s \n",
            "\u001b[?25h  Building wheel for dalle-mini (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UyG35bnzZRR",
        "outputId": "c12fb593-cba6-4493-d0ef-2ca1ae548c88"
      },
      "source": [
        "!pip install flax==0.3.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flax==0.3.5\n",
            "  Downloading flax-0.3.5-py3-none-any.whl (193 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▊                              | 10 kB 17.6 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████                           | 30 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 61 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 71 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 81 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 92 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 102 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 112 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 122 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 133 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 143 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 153 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 163 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 174 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 184 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 193 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: optax in /usr/local/lib/python3.7/dist-packages (from flax==0.3.5) (0.1.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from flax==0.3.5) (1.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from flax==0.3.5) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from flax==0.3.5) (1.19.5)\n",
            "Requirement already satisfied: jax>=0.2.13 in /usr/local/lib/python3.7/dist-packages (from flax==0.3.5) (0.2.25)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.13->flax==0.3.5) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.13->flax==0.3.5) (1.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.13->flax==0.3.5) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.13->flax==0.3.5) (3.10.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax>=0.2.13->flax==0.3.5) (1.15.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax==0.3.5) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax==0.3.5) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax==0.3.5) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax==0.3.5) (2.8.2)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax->flax==0.3.5) (0.1.74+cuda11.cudnn805)\n",
            "Requirement already satisfied: chex>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from optax->flax==0.3.5) (0.1.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax==0.3.5) (0.1.6)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax==0.3.5) (0.11.2)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib>=0.1.37->optax->flax==0.3.5) (2.0)\n",
            "Installing collected packages: flax\n",
            "  Attempting uninstall: flax\n",
            "    Found existing installation: flax 0.3.6\n",
            "    Uninstalling flax-0.3.6:\n",
            "      Successfully uninstalled flax-0.3.6\n",
            "Successfully installed flax-0.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1nLZzDfzZWB",
        "outputId": "c600f771-afcd-4b32-9a7c-0bf3c02a62f5"
      },
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (6.0.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-jvobzo2c\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-jvobzo2c\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.0.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.62.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.11.1+cu111)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369106 sha256=a6686c09b5da1f1707cd23fd901f39ff33baf352423bfd42d8834f22ee2535bf\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-69lt9ok_/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xuK8vmG1KRD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idEzD8V61KVF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsESth7jzSck"
      },
      "source": [
        "from dalle_mini.model import CustomFlaxBartForConditionalGeneration\n",
        "from transformers import BartTokenizer\n",
        "import jax\n",
        "import random\n",
        "from tqdm.notebook import tqdm, trange\n",
        "\n",
        "from vqgan_jax.modeling_flax_vqgan import VQModel\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "from transformers import CLIPProcessor, FlaxCLIPModel, CLIPModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRJfxxtEzB8H"
      },
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Function\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "from torchvision.transforms import ToPILImage\n",
        "from torchvision.transforms import Compose, Resize, InterpolationMode, Normalize\n",
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "\n",
        "import easydict\n",
        "import cv2\n",
        "\n",
        "import os\n",
        "from glob import glob\n",
        "import pickle\n",
        "import random\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import normalize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiWNUc15q_gM"
      },
      "source": [
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "           'resnet152']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth'\n",
        "}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, in_channels=12):\n",
        "        self.inplanes = 16\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=7, stride=1, padding=3,\n",
        "                               bias=False) # ori : stride = 2\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 128, layers[3], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(1, stride=1)\n",
        "        self.fc = nn.Linear(128 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def resnet18(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet34(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet50(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-50 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet101(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet152(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-152 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU8bcixC0Qfp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5xhBCkGzJTt"
      },
      "source": [
        "# dataloader\n",
        "class IMG_Dataset(Dataset):\n",
        "    def __init__(self, args, is_validated=False):\n",
        "        self.args = args\n",
        "        if is_validated == True:\n",
        "          self.args.mode = 'val'\n",
        "\n",
        "        data_path = os.path.join('/content/gdrive/MyDrive/Colab Notebooks/data', args.mode) + '.pkl'\n",
        "        prompts_path = os.path.join('/content/gdrive/MyDrive/Colab Notebooks/data', args.mode) + '_prompts.pkl'\n",
        "\n",
        "        if os.path.isfile(data_path):\n",
        "            with open(data_path, 'rb') as f:\n",
        "                self.data = pickle.load(f)\n",
        "                print('data num: ', len(self.data))\n",
        "        else:\n",
        "            self.data = list()\n",
        "            images = list()\n",
        "            prompts = list()\n",
        "            prompt2idx = dict()\n",
        "            idx2prompt = dict()\n",
        "\n",
        "            texts_path = glob(os.path.join('/content/gdrive/MyDrive/Colab Notebooks/corpus', args.mode)+'/*')\n",
        "            for text in texts_path:\n",
        "              i = 0\n",
        "              images_path = glob(text + '/*')\n",
        "              temp = []\n",
        "              for image in images_path:\n",
        "                i+=1\n",
        "                img = cv2.imread(image)\n",
        "                img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
        "                temp.append(img)\n",
        "                if i == 2:\n",
        "                  break\n",
        "                \n",
        "              prompt = images_path[0].split('/')[-2].replace('_', ' ')\n",
        "              print(prompt)\n",
        "              prompts.append(prompt)\n",
        "\n",
        "              temp = np.array(temp)\n",
        "              images.append(temp)\n",
        "            \n",
        "            prompt2idx = {prompt:i for i,prompt in enumerate(sorted(prompts))}\n",
        "            idx2prompt = {prompt:i for prompt,i in enumerate(sorted(prompts))}\n",
        "            prompts = [prompt2idx[prompt] for prompt in prompts]\n",
        "\n",
        "            for image, prompt in zip(images, prompts):\n",
        "              self.data.append((np.array([image, prompt]), 0))\n",
        "\n",
        "            print(len(self.data))\n",
        "            print(self.data[0][0].shape)\n",
        "\n",
        "            with open(prompts_path, 'wb') as f:\n",
        "                pickle.dump(idx2prompt, f)\n",
        "            with open(data_path, 'wb') as f:\n",
        "                pickle.dump(self.data, f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        x = self.data[i][0][0]\n",
        "        prompt = self.data[i][0][1]\n",
        "        y = self.data[i][1]\n",
        "        return x, prompt, y\n",
        "\n",
        "class IMG_Loader(object):\n",
        "    def __init__(self, args, is_validated=False):\n",
        "        super().__init__()\n",
        "\n",
        "        dataset = IMG_Dataset(args)\n",
        "\n",
        "        if args.mode == 'train':\n",
        "            self.data_loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
        "        else:\n",
        "            self.data_loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size)\n",
        "        self.data_iter = self.data_loader.__iter__()\n",
        "    \n",
        "    def next_batch(self):\n",
        "        try:\n",
        "            batch = self.data_iter.__next__()\n",
        "        except StopIteration:\n",
        "            self.data_iter = self.data_loader.__iter__()\n",
        "            batch = self.data_iter.__next__()\n",
        "        \n",
        "        return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoZ9syWhzJWF"
      },
      "source": [
        "# trainer\n",
        "class Trainer():\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.loader = IMG_Loader(args)\n",
        "        #self.model = MLP(args)\n",
        "        self.model = CNN(args)\n",
        "        #self.model = background_resnet(embedding_size=256)\n",
        "        if self.args.device == 'cuda':\n",
        "          self.model.cuda()\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optim = torch.optim.Adam(self.model.params_to_update)\n",
        "        \n",
        "    def train(self):\n",
        "        # train\n",
        "        total_step = len(self.loader.data_loader)\n",
        "        #val_loader = IMG_Loader(self.args, is_validated=True)\n",
        "        opt_epoch = 1\n",
        "        min_val_loss = 1e9\n",
        "\n",
        "        print(total_step)\n",
        "        for epoch in range(self.args.epoch):\n",
        "            self.model.train()\n",
        "            total_loss = 0\n",
        "\n",
        "            for i in range(len(self.loader.data_loader)):\n",
        "                feature, prompt, label = self.loader.next_batch()\n",
        "                feature = torch.tensor(feature).to(device=self.args.device, dtype=torch.float)\n",
        "                label = torch.tensor(label).to(device=self.args.device)\n",
        "\n",
        "                pred = self.model(feature, prompt)\n",
        "                loss = self.criterion(pred, label)\n",
        "                \n",
        "                self.optim.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optim.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                if (i + 1) % 5 == 0:\n",
        "                    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, self.args.epoch, i + 1, total_step, loss.item()))\n",
        "\n",
        "            #if (epoch+1)%50 == 0:\n",
        "            '''\n",
        "            val_loss = self.validate(val_loader)\n",
        "            print ('Validation Loss: {:.20f}'.format(val_loss))\n",
        "            if val_loss < min_val_loss:\n",
        "              val_loss = min_val_loss\n",
        "              opt_epoch = epoch+1\n",
        "            '''\n",
        "            torch.save(self.model.state_dict(), '/content/gdrive/MyDrive/Colab Notebooks/model_saved/checkpoint_'+str(epoch+1)+'.pt')\n",
        "\n",
        "    def validate(self, val_loader=None):\n",
        "        if val_loader is None:\n",
        "          val_loader = IMG_Loader(self.args, is_validated=True)\n",
        "        total_loss = 0\n",
        "        total_step = len(val_loader.data_loader)\n",
        "\n",
        "        self.model.load_state_dict(torch.load('/content/gdrive/MyDrive/Colab Notebooks/model_saved/checkpoint_2.pt'))\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i in range(len(val_loader.data_loader)):\n",
        "                feature, prompt, label = self.loader.next_batch()\n",
        "                feature = torch.tensor(feature).to(device=self.args.device, dtype=torch.float)\n",
        "                label = torch.tensor(label).to(device=self.args.device)\n",
        "\n",
        "                pred = self.model(feature, prompt)\n",
        "                loss = self.criterion(pred, label)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                if (i + 1) % 5 == 0:\n",
        "                    print ('Validation Step [{}/{}], Loss: {:.4f}'.format(i + 1, total_step, loss.item()))\n",
        "\n",
        "        return total_loss/total_step\n",
        "\n",
        "\n",
        "    def test(self):\n",
        "        # test\n",
        "        #self.model.load_state_dict(torch.load(self.args.ckpt))\n",
        "        self.model.load_state_dict(torch.load('/content/gdrive/MyDrive/Colab Notebooks/model_saved/checkpoint_6.pt', map_location=torch.device('cpu')))\n",
        "        self.model.eval()\n",
        "\n",
        "        pred_list = list()\n",
        "        label_list = list()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(len(self.loader.data_loader)):\n",
        "                feature, label = self.loader.next_batch()\n",
        "                feature = torch.tensor(feature).to(device=self.args.device, dtype=torch.float)\n",
        "                label = torch.tensor(label).to(device=self.args.device)\n",
        "\n",
        "                pred = self.model(feature)\n",
        "                pred = F.softmax(pred, dim=1)\n",
        "                pred = torch.argmax(pred, dim=1)\n",
        "                \n",
        "                pred_list.extend(pred.tolist())\n",
        "                label_list.extend(label.tolist())\n",
        "\n",
        "        acc = accuracy_score(label_list, pred_list)\n",
        "        print(acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQdhuG6yrJ67"
      },
      "source": [
        "class background_resnet(nn.Module):\n",
        "    def __init__(self, embedding_size, backbone='resnet18'):\n",
        "        super(background_resnet, self).__init__()\n",
        "        self.args = args\n",
        "        self.prompt_path = os.path.join('/content/gdrive/MyDrive/Colab Notebooks/data', args.mode) + '_prompts.pkl'\n",
        "        with open(self.prompt_path, 'rb') as f:\n",
        "          self.idx2prompt = pickle.load(f)\n",
        "        print(self.idx2prompt)\n",
        "\n",
        "        self.my_preprocess = Compose([\n",
        "                                Resize(224, interpolation=InterpolationMode.BICUBIC),\n",
        "                                Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "                            ])\n",
        "        self.model, _ = clip.load(\"ViT-B/32\", device=self.args.device)\n",
        "\n",
        "        self.backbone = backbone\n",
        "        # copying modules from pretrained models\n",
        "        if backbone == 'resnet50':\n",
        "            self.pretrained = resnet50(pretrained=False)\n",
        "        elif backbone == 'resnet101':\n",
        "            self.pretrained = resnet101(pretrained=False)\n",
        "        elif backbone == 'resnet152':\n",
        "            self.pretrained = resnet152(pretrained=False)\n",
        "        elif backbone == 'resnet18':\n",
        "            self.pretrained = resnet18(pretrained=False)\n",
        "        elif backbone == 'resnet34':\n",
        "            self.pretrained = resnet34(pretrained=False)\n",
        "        else:\n",
        "            raise RuntimeError('unknown backbone: {}'.format(backbone))\n",
        "            \n",
        "        self.fc0 = nn.Linear(128, embedding_size)\n",
        "        #self.bn0 = nn.BatchNorm1d(embedding_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.last = nn.Linear(embedding_size, 150528)\n",
        "        \n",
        "        self.params_to_update = list(self.pretrained.parameters()) + list(self.fc0.parameters()) + list(self.last.parameters())\n",
        "\n",
        "    def forward(self, x, prompts):\n",
        "        prompts = prompts.tolist()\n",
        "        prompts = [self.idx2prompt[prompt] for prompt in prompts]\n",
        "        prompt = prompts[0]\n",
        "\n",
        "        out = x.permute(0,1,4,2,3)\n",
        "        out = out.reshape(1,-1,224,224)\n",
        "\n",
        "        out = self.pretrained.conv1(out)\n",
        "        out = self.pretrained.bn1(out)\n",
        "        out = self.pretrained.relu(out)\n",
        "        \n",
        "        out = self.pretrained.layer1(out)\n",
        "        out = self.pretrained.layer2(out)\n",
        "        out = self.pretrained.layer3(out)\n",
        "        out = self.pretrained.layer4(out)\n",
        "        \n",
        "        out = F.adaptive_avg_pool2d(out,1) # [batch, 128, 1, 1]\n",
        "        out = torch.squeeze(out) # [batch, n_embed]\n",
        "        # flatten the out so that the fully connected layer can be connected from here\n",
        "        out = out.view(x.size(0), -1) # (n_batch, n_embed)\n",
        "        spk_embedding = self.fc0(out)\n",
        "        out = F.relu(spk_embedding) # [batch, n_embed]\n",
        "        out = self.last(out)\n",
        "\n",
        "        out = out.uniform_(0,1)*255\n",
        "        out = out.reshape(x.size(0),1,224,224,3)\n",
        "\n",
        "        if self.args.mode != 'train':\n",
        "          cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/output/'+prompt+'.png', (torch.squeeze(out)).cpu().numpy())\n",
        "          cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/output/'+prompt+'_0_.png', (torch.squeeze(x)[0]).cpu().numpy())\n",
        "          cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/output/'+prompt+'_1_.png', (torch.squeeze(x)[1]).cpu().numpy())\n",
        "          cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/output/'+prompt+'_2_.png', (torch.squeeze(x)[2]).cpu().numpy())\n",
        "          cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/output/'+prompt+'_3_.png', (torch.squeeze(x)[3]).cpu().numpy())\n",
        "        \n",
        "        out = torch.squeeze(out)\n",
        "        out = torch.unsqueeze(out, dim=0).permute(0,3,1,2)\n",
        "        x = torch.squeeze(x).permute(0,3,1,2)\n",
        "\n",
        "        # evaluate scores\n",
        "        images = torch.cat([self.my_preprocess(out), self.my_preprocess(x)])\n",
        "        text = torch.cat([clip.tokenize([prompt]).to(self.args.device)])\n",
        "        logits_per_image, logits_per_text = self.model(images, text)\n",
        "\n",
        "        logits = torch.squeeze(logits_per_image)\n",
        "        logits = logits.reshape(1,-1)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bCMfmDWzJYj"
      },
      "source": [
        "# MLP\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(MLP, self).__init__()\n",
        "        self.args = args\n",
        "        self.prompt_path = os.path.join('/content/gdrive/MyDrive/Colab Notebooks/data', args.mode) + '_prompts.pkl'\n",
        "\n",
        "        with open(self.prompt_path, 'rb') as f:\n",
        "          self.idx2prompt = pickle.load(f)\n",
        "        print(self.idx2prompt)\n",
        "        self.layer1 = nn.Linear(301056, 128)\n",
        "        self.layer2 = nn.Linear(128, 150528)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.my_preprocess = Compose([\n",
        "                                Resize(224, interpolation=InterpolationMode.BICUBIC),\n",
        "                                Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "                            ])\n",
        "        self.model, _ = clip.load(\"ViT-B/32\", device=self.args.device)\n",
        "        # set up model and processor\n",
        "        #self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        #self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", do_resize=False, do_center_crop=False)\n",
        "        \n",
        "        self.params_to_update = list(self.layer1.parameters()) + list(self.layer2.parameters())\n",
        "\n",
        "    def forward(self, x, prompts):\n",
        "        prompts = prompts.tolist()\n",
        "        prompts = [self.idx2prompt[prompt] for prompt in prompts]\n",
        "        prompt = prompts[0]\n",
        "\n",
        "        print('x: ', x.shape)\n",
        "\n",
        "        out = x.reshape(x.size(0), -1)\n",
        "        out = self.layer1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer2(out)\n",
        "        out = out.uniform_(0,1)*255\n",
        "        out = out.reshape(x.size(0),1,224,224,3)\n",
        "\n",
        "        if self.args.mode != 'train':\n",
        "          cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/output/'+prompt+'.png', (torch.squeeze(out)).cpu().numpy())\n",
        "          cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/output/'+prompt+'_0_.png', (torch.squeeze(x)[0]).cpu().numpy())\n",
        "          cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/output/'+prompt+'_1_.png', (torch.squeeze(x)[1]).cpu().numpy())\n",
        "          cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/output/'+prompt+'_2_.png', (torch.squeeze(x)[2]).cpu().numpy())\n",
        "          cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/output/'+prompt+'_3_.png', (torch.squeeze(x)[3]).cpu().numpy())\n",
        "\n",
        "        out = torch.squeeze(out)\n",
        "        out = torch.unsqueeze(out, dim=0).permute(0,3,1,2)\n",
        "        x = torch.squeeze(x).permute(0,3,1,2)\n",
        "\n",
        "        # evaluate scores\n",
        "        images = torch.cat([self.my_preprocess(out), self.my_preprocess(x)])\n",
        "        text = torch.cat([clip.tokenize([prompt]).to(self.args.device)])\n",
        "        logits_per_image, logits_per_text = self.model(images, text)\n",
        "\n",
        "        logits = torch.squeeze(logits_per_image)\n",
        "        logits = logits.reshape(1,-1)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOo9CUW0zIRL"
      },
      "source": [
        "# model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(CNN, self).__init__()\n",
        "        self.args = args\n",
        "        self.prompt_path = os.path.join('/content/gdrive/MyDrive/Colab Notebooks/data', args.mode) + '_prompts.pkl'\n",
        "        with open(self.prompt_path, 'rb') as f:\n",
        "          self.idx2prompt = pickle.load(f)\n",
        "        print(self.idx2prompt)\n",
        "\n",
        "        self.my_preprocess = Compose([\n",
        "                                Resize(224, interpolation=InterpolationMode.BICUBIC),\n",
        "                                Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "                            ])\n",
        "        self.model, _ = clip.load(\"ViT-B/32\", device=self.args.device)\n",
        "\n",
        "        # (64,64,1) => (32,32,16) => (16,16,16) => (8,8,32) => (4,4,64)\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(12, 16, kernel_size=6, stride=2, padding=2),\n",
        "            nn.Dropout(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=1, stride=1))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 16, kernel_size=6, stride=2, padding=2),\n",
        "            nn.Dropout(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=1, stride=1))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=6, stride=2, padding=2),\n",
        "            nn.Dropout(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=1, stride=1))\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=6, stride=2, padding=2),\n",
        "            nn.Dropout(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=1, stride=1))\n",
        "        self.fc1 = nn.Linear(12544, 128)\n",
        "        self.fc2 = nn.Linear(128, 150528)\n",
        "\n",
        "        \n",
        "        self.params_to_update = list(self.layer1.parameters()) + list(self.layer2.parameters()) + list(self.layer3.parameters()) + list(self.layer4.parameters()) + list(self.fc1.parameters()) + list(self.fc2.parameters())\n",
        "\n",
        "    def forward(self, x, prompts):\n",
        "        prompts = prompts.tolist()\n",
        "        prompts = [self.idx2prompt[prompt] for prompt in prompts]\n",
        "        prompt = prompts[0]\n",
        "\n",
        "        out = x.permute(0,1,4,2,3)\n",
        "        out = out.reshape(1,-1,224,224)\n",
        "\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        out = out.uniform_(0,1)*255\n",
        "        out = out.reshape(x.size(0),1,224,224,3)\n",
        "\n",
        "        if self.args.mode != 'train':\n",
        "          cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/output/'+prompt+'.png', (torch.squeeze(out)).cpu().numpy())\n",
        "          cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/output/'+prompt+'_0_.png', (torch.squeeze(x)[0]).cpu().numpy())\n",
        "          cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/output/'+prompt+'_1_.png', (torch.squeeze(x)[1]).cpu().numpy())\n",
        "          cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/output/'+prompt+'_2_.png', (torch.squeeze(x)[2]).cpu().numpy())\n",
        "          cv2.imwrite('/content/gdrive/MyDrive/Colab Notebooks/output/'+prompt+'_3_.png', (torch.squeeze(x)[3]).cpu().numpy())\n",
        "        \n",
        "        out = torch.squeeze(out)\n",
        "        out = torch.unsqueeze(out, dim=0).permute(0,3,1,2)\n",
        "        x = torch.squeeze(x).permute(0,3,1,2)\n",
        "\n",
        "        # evaluate scores\n",
        "        images = torch.cat([self.my_preprocess(out), self.my_preprocess(x)])\n",
        "        text = torch.cat([clip.tokenize([prompt]).to(self.args.device)])\n",
        "        logits_per_image, logits_per_text = self.model(images, text)\n",
        "\n",
        "        logits = torch.squeeze(logits_per_image)\n",
        "        logits = logits.reshape(1,-1)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhJJHyfrzJa5"
      },
      "source": [
        "# args\n",
        "def get_args():\n",
        "    args = easydict.EasyDict({\n",
        "        \"epoch\": 30,\n",
        "        \"batch_size\": 1,\n",
        "        \"mode\": 'train',\n",
        "        \"ckpt\": 1,\n",
        "        \"device\": 'cuda'\n",
        "    })\n",
        "    return args"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAcvWIpDzJc9",
        "outputId": "f042d868-f7e4-4239-f2a4-578aa5cb4f34"
      },
      "source": [
        "# model run\n",
        "args = get_args()\n",
        "trainer = Trainer(args)\n",
        "\n",
        "if args.mode == 'train':\n",
        "  trainer.train()\n",
        "else:\n",
        "  trainer.validate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data num:  183\n",
            "{0: 'A spoon in a pot of fry sauce', 1: 'Animals at the guesthouse or nearby', 2: 'Art Of A Little Girl In A Blu', 3: 'Drink Like A Pirate Dance Like A Mermaid Poster', 4: 'Food at or somewhere near the hotel', 5: 'Image of Stained Concrete Flooring Throughout for The Towers Seabrook', 6: 'Lavender Flower Watercolor Yellow Save the Date Announcement', 7: 'Mind the Shoes city girl', 8: 'Nuts on a Keto Diet', 9: 'Pin By The Holiday Cottages On Wonderful Wales Cottage House', 10: 'Portrait of the young smiling sportsman stock photos', 11: 'Relaxing outside the Lamington Tea House at Binna Burra', 12: 'Sea spiders stacked on top of one another', 13: 'Teenage girl playing on a scooter stock photography', 14: 'The Big Bad Theory Throw Pillow', 15: 'Winter at the Grand Canyon Sticker', 16: 'a beautiful day with some buildings and plants ', 17: 'a blog about home and garden design including design ', 18: 'a city for sale property ', 19: 'a flower photographed from above ', 20: 'a giant pumpkin grown by person ', 21: 'a llama i  ve named person gazes at the ruins in the waning sunlight ', 22: 'a long table is laid at the deserted dining room', 23: 'a map of the natural gas and water pipeline leading', 24: 'a mountain goat atop mt ', 25: 'a woman walks her dog on the beach ', 26: 'abstract background for the arrival of new year illustration', 27: 'accommodation type for rent in an apartment - flat', 28: 'actor arrives at the premiere', 29: 'actor arrives for the premiere', 30: 'actor attends the film premiere at theater', 31: 'actor attends the premiere during festival ', 32: 'actor attends the season premiere', 33: 'actor attends the world premiere', 34: 'actor holds a basketball while sitting on the set of romantic comedy film', 35: 'actor of battles with football player during football league sponsored by match ', 36: 'add a zipper to existing sweater for visual interest ', 37: 'after the fall - the riots', 38: 'all in top down and starting out dress', 39: 'an amazing view from the hammock ', 40: 'an oil painting of a rain - soaked windshield on a highway ', 41: 'animal spotted on the loose on a busy road', 42: 'another possible invitation -- love the font , just need to switch to coral and aqua colors', 43: 'apples in the basket , sun , summer', 44: 'artist , lead singer , dressed as a priest , performs pop artist', 45: 'artwork is an oil painting by painting artist , dating to 1508', 46: 'automobile model of all time !', 47: 'baggage , a very large pile of bags , backpacks and suitcases', 48: 'basketball player and woman attend premiere during festival ', 49: 'basketball players celebrate during a game against basketball team ', 50: 'bathrooms in the rental home', 51: 'beverage that scores better than wineries business and vintage - at half the price ', 52: 'black cat in front of a white background', 53: 'breakfast on the second morning ', 54: 'build kindergarten number sense with these differentiated centers !', 55: 'building function is a popular spot for special events ', 56: 'car parked in front of the classic architecture', 57: 'casual man walking - isolated over a white background', 58: 'celebrating food friday like country !', 59: 'check out the best coffee shops', 60: 'christmas tree in the white room new year', 61: 'christmas tree on a black background ', 62: 'close up of the face of a boy', 63: 'close up portrait of a smiling middle aged woman sitting against white wall', 64: 'cupcakes from food greet us', 65: 'cutting out an even edge for transitional piece between old and new floors', 66: 'death metal artist open the concert for heavy metal artist', 67: 'depressed little boy sitting on the floor', 68: 'digital art selected for the #', 69: 'elevators in a modern office building', 70: 'ethnic person , the foot with thermal bag ', 71: 'event wears a costume on stage', 72: 'everyone needs a deck -- a place covered with wooden planks , kind of a terrace somewhere in your garden', 73: 'facade turned to the lagoon ', 74: 'faceless scary man in a hood against a dark background ', 75: 'female puppies for person is animal , person is animal ', 76: 'filming location from the river', 77: 'first thing in the morning', 78: 'flag on top of a dome', 79: 'food was passed through the small insert', 80: 'footballer acknowledges the fans after the match', 81: 'general view as fans wave flags during the match ', 82: 'giraffes explore the newly extended plains of enclosure ', 83: 'gold jewelry for sale in a market royalty - free', 84: 'golfer during sports league championship held', 85: 'greek dish are made with simple ingredients and have big flavor ', 86: 'grow to love yourself you are the person you spend every waking minute with ', 87: 'hairstyles - deal of person', 88: 'hard rock artist performs on stage he plays a guitar', 89: 'hiking along several trails', 90: 'i remember playing with these at my grandma  s house when i was a little girl !', 91: 'i will give you all of me ', 92: 'illustration of a cute cat on a meadow', 93: 'illustration of an isolated map icon with a broken heart', 94: 'image may contain   person , on stage , playing a musical instrument and indoor', 95: 'image may contain   person , riding on a horses', 96: 'industry , red plate on the pole', 97: 'interior of a room for game in billiards', 98: 'isolated flag on a badge , vector illustration illustration', 99: 'item   drawing of a figure surrounded by person', 100: 'knowing the royals   wedding dress of olympic athlete', 101: 'larch trees under the sun in fall ', 102: 'looking out from the ruins', 103: 'make sure to sample complimentary homemade fudge !', 104: 'man sitting on floor beside a pool using laptop', 105: 'maneuvers between defenders during a game against state school ', 106: 'meadow with yellow dandelions beside a lake ', 107: 'medium sized brown bird with white speckled chest perched on a tree branch', 108: 'members getting ready to sail on the sea', 109: 'music as hard rock artist , heavy metal artist and alternative metal artist', 110: 'naruto the coat + pant + vest + top + bag can custom for you', 111: 'national holiday day grainy textured icon for overlay watermark stamps ', 112: 'never separate the top and bottom of the vampire , that is what it needs to grow the wings ', 113: 'new and modern medical table on a white ', 114: 'nice picture of apples , strawberries , black currants and juice rich in vitamin c on a white background', 115: 'nothing brings the world together like independence ', 116: 'once it  s all set , person held it in place with some temporary wood shoring ', 117: 'peace - that was the other name for home ', 118: 'pen and ink portrait drawing of a child', 119: 'people enjoying the view as the clouds parted ', 120: 'people pose with shirts designed for the event', 121: 'people were gems hanging above the celebrated skyline', 122: 'person , runs with the ball during practice ', 123: 'person after riding in the mud', 124: 'person arrives at the premiere', 125: 'person attends the promotional event ', 126: 'person in boots -- a country wedding --', 127: 'person looked elegant , refined , and so fashion - forward in her floral gown ', 128: 'person looking at the top of the fence', 129: 'person of the haunted woods', 130: 'person stands on the podium , draped in the flag , after winning gold during competition ', 131: 'photo of a front garden with white flowers', 132: 'photograph of the opening party of premises ', 133: 'picnic styled tables arranged for a meal ', 134: 'picture of a waterfall with full moon', 135: 'pin for later   favorite jeans are stuffed somewhere playing up a bleach - splattered pair with a cobalt - blue bomber jacket and satchel ', 136: 'pop artist performs during the media call', 137: 'portrait of a happy father and daughter ', 138: 'portrait of the bride and groom', 139: 'pricing for stickers is per set ', 140: 'print with black brush strokes on a gold background ', 141: 'residents sunbathe on beach during an extreme heatwave , with temperatures reaching degrees for consecutive days', 142: 'rich golden metallic beveled lowercase or small letter y in a 3d illustration with a deep gold color and shiny metal surface with a classic font isolated on a white background with clipping path .', 143: 'river running through the snow', 144: 'romantic card with bear , fox and a girl hugging on a background with stars ', 145: 'school category welcomes year pupils', 146: 'seamless abstract black red texture fractal patterns on white background ', 147: 'set of multicolored buttons for the site ', 148: 'sets of doors open this dining room to a lovely outdoor setting , while dark curtains provide drama and frame the view ', 149: 'sheriff  s deputies search a vehicle after people were taken into custody on drug and other charges ', 150: 'small gray kitten on a wooden background', 151: 'soccer ball on a yellow - green background -- stock vector #', 152: 'soccer player celebrates scoring his team  s second goal with his team mates during the match ', 153: 'sports team are expected to host a game which means american football team could be one of their opponents ', 154: 'staff walk to air force one ', 155: 'stock image of scenery showing a bird named', 156: 'tennis players on court during the charity match on day ', 157: 'the beach at the resort', 158: 'the cat in knitted hat and scarf is holding a cup of black coffee ', 159: 'the grass is always greener until you have to mow it ', 160: 'the men  s national team in early october', 161: 'the number of suits sold per day at a retail store', 162: 'the pound in your pocket', 163: 'the sidewalk near the corner of streets has one of the few vending machines ', 164: 'the value of old olive trees ', 165: 'the view of animal on the balloon', 166: 'this is mostly geometric , so what does the tree do -- serve as variety ', 167: 'this is that kind of tattoo that you can call letter ', 168: 'tourist attraction in the snow', 169: 'tourist attraction under the snow', 170: 'tv actor attends the premiere ', 171: 'under a weeping cherry blossom tree', 172: 'unesco world heritage site like statue ', 173: 'use food coloring to change the color of the rice ', 174: 'using shrubs and hedges as fences is a very popular use for them ', 175: 'vector illustration of a poster or banner for celebration ', 176: 'visitors get a close - up view of a helicopter', 177: 'waves to the massed crowds as republic celebrated their incredible achievement', 178: 'we have been learning about a farmer  s job and farm animals ', 179: 'wedding shoes from the 19th century ', 180: 'west coast hip hop artist performs onstage', 181: 'what kind of bird is this  ', 182: 'which of young on - loan stars can break into the first team next season  '}\n",
            "183\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Step [5/183], Loss: 4.5273\n",
            "Epoch [1/30], Step [10/183], Loss: 3.3926\n",
            "Epoch [1/30], Step [15/183], Loss: 2.5039\n",
            "Epoch [1/30], Step [20/183], Loss: 1.1992\n",
            "Epoch [1/30], Step [25/183], Loss: 4.9609\n",
            "Epoch [1/30], Step [30/183], Loss: 4.4414\n",
            "Epoch [1/30], Step [35/183], Loss: 2.9746\n",
            "Epoch [1/30], Step [40/183], Loss: 3.5996\n",
            "Epoch [1/30], Step [45/183], Loss: 5.3750\n",
            "Epoch [1/30], Step [50/183], Loss: 2.0371\n",
            "Epoch [1/30], Step [55/183], Loss: 3.3633\n",
            "Epoch [1/30], Step [60/183], Loss: 2.3730\n",
            "Epoch [1/30], Step [65/183], Loss: 6.0938\n",
            "Epoch [1/30], Step [70/183], Loss: 4.3320\n",
            "Epoch [1/30], Step [75/183], Loss: 5.3086\n",
            "Epoch [1/30], Step [80/183], Loss: 3.8789\n",
            "Epoch [1/30], Step [85/183], Loss: 5.5781\n",
            "Epoch [1/30], Step [90/183], Loss: 5.4102\n",
            "Epoch [1/30], Step [95/183], Loss: 1.4824\n",
            "Epoch [1/30], Step [100/183], Loss: 2.0215\n",
            "Epoch [1/30], Step [105/183], Loss: 3.5938\n",
            "Epoch [1/30], Step [110/183], Loss: 3.6523\n",
            "Epoch [1/30], Step [115/183], Loss: 3.7344\n",
            "Epoch [1/30], Step [120/183], Loss: 3.2539\n",
            "Epoch [1/30], Step [125/183], Loss: 3.5254\n",
            "Epoch [1/30], Step [130/183], Loss: 3.6602\n",
            "Epoch [1/30], Step [135/183], Loss: 2.1055\n",
            "Epoch [1/30], Step [140/183], Loss: 4.7578\n",
            "Epoch [1/30], Step [145/183], Loss: 4.2617\n",
            "Epoch [1/30], Step [150/183], Loss: 2.8281\n",
            "Epoch [1/30], Step [155/183], Loss: 3.5781\n",
            "Epoch [1/30], Step [160/183], Loss: 9.8125\n",
            "Epoch [1/30], Step [165/183], Loss: 1.2393\n",
            "Epoch [1/30], Step [170/183], Loss: 3.2305\n",
            "Epoch [1/30], Step [175/183], Loss: 1.5674\n",
            "Epoch [1/30], Step [180/183], Loss: 1.9248\n",
            "Epoch [2/30], Step [5/183], Loss: 5.4141\n",
            "Epoch [2/30], Step [10/183], Loss: 3.3320\n",
            "Epoch [2/30], Step [15/183], Loss: 4.7227\n",
            "Epoch [2/30], Step [20/183], Loss: 6.5312\n",
            "Epoch [2/30], Step [25/183], Loss: 2.9551\n",
            "Epoch [2/30], Step [30/183], Loss: 2.1660\n",
            "Epoch [2/30], Step [35/183], Loss: 3.2930\n",
            "Epoch [2/30], Step [40/183], Loss: 3.4355\n",
            "Epoch [2/30], Step [45/183], Loss: 2.5469\n",
            "Epoch [2/30], Step [50/183], Loss: 2.8438\n",
            "Epoch [2/30], Step [55/183], Loss: 2.3730\n",
            "Epoch [2/30], Step [60/183], Loss: 2.5879\n",
            "Epoch [2/30], Step [65/183], Loss: 3.2012\n",
            "Epoch [2/30], Step [70/183], Loss: 3.6113\n",
            "Epoch [2/30], Step [75/183], Loss: 6.3984\n",
            "Epoch [2/30], Step [80/183], Loss: 3.8633\n",
            "Epoch [2/30], Step [85/183], Loss: 2.3691\n",
            "Epoch [2/30], Step [90/183], Loss: 4.6250\n",
            "Epoch [2/30], Step [95/183], Loss: 7.8516\n",
            "Epoch [2/30], Step [100/183], Loss: 3.0781\n",
            "Epoch [2/30], Step [105/183], Loss: 3.5859\n",
            "Epoch [2/30], Step [110/183], Loss: 1.1309\n",
            "Epoch [2/30], Step [115/183], Loss: 2.3809\n",
            "Epoch [2/30], Step [120/183], Loss: 5.2070\n",
            "Epoch [2/30], Step [125/183], Loss: 3.1445\n",
            "Epoch [2/30], Step [130/183], Loss: 3.3086\n",
            "Epoch [2/30], Step [135/183], Loss: 2.6777\n",
            "Epoch [2/30], Step [140/183], Loss: 5.1289\n",
            "Epoch [2/30], Step [145/183], Loss: 5.6133\n",
            "Epoch [2/30], Step [150/183], Loss: 3.6582\n",
            "Epoch [2/30], Step [155/183], Loss: 3.5449\n",
            "Epoch [2/30], Step [160/183], Loss: 1.7393\n",
            "Epoch [2/30], Step [165/183], Loss: 3.5742\n",
            "Epoch [2/30], Step [170/183], Loss: 3.3203\n",
            "Epoch [2/30], Step [175/183], Loss: 3.9668\n",
            "Epoch [2/30], Step [180/183], Loss: 2.1465\n",
            "Epoch [3/30], Step [5/183], Loss: 5.2656\n",
            "Epoch [3/30], Step [10/183], Loss: 3.3203\n",
            "Epoch [3/30], Step [15/183], Loss: 5.1523\n",
            "Epoch [3/30], Step [20/183], Loss: 2.9590\n",
            "Epoch [3/30], Step [25/183], Loss: 4.1680\n",
            "Epoch [3/30], Step [30/183], Loss: 1.8115\n",
            "Epoch [3/30], Step [35/183], Loss: 2.0859\n",
            "Epoch [3/30], Step [40/183], Loss: 7.5273\n",
            "Epoch [3/30], Step [45/183], Loss: 3.4512\n",
            "Epoch [3/30], Step [50/183], Loss: 1.8555\n",
            "Epoch [3/30], Step [55/183], Loss: 4.1406\n",
            "Epoch [3/30], Step [60/183], Loss: 1.4951\n",
            "Epoch [3/30], Step [65/183], Loss: 2.3066\n",
            "Epoch [3/30], Step [70/183], Loss: 4.8672\n",
            "Epoch [3/30], Step [75/183], Loss: 2.8672\n",
            "Epoch [3/30], Step [80/183], Loss: 3.3691\n",
            "Epoch [3/30], Step [85/183], Loss: 3.4590\n",
            "Epoch [3/30], Step [90/183], Loss: 3.0039\n",
            "Epoch [3/30], Step [95/183], Loss: 2.3535\n",
            "Epoch [3/30], Step [100/183], Loss: 3.0586\n",
            "Epoch [3/30], Step [105/183], Loss: 4.3594\n",
            "Epoch [3/30], Step [110/183], Loss: 4.8164\n",
            "Epoch [3/30], Step [115/183], Loss: 1.3398\n",
            "Epoch [3/30], Step [120/183], Loss: 6.8203\n",
            "Epoch [3/30], Step [125/183], Loss: 2.7324\n",
            "Epoch [3/30], Step [130/183], Loss: 3.9316\n",
            "Epoch [3/30], Step [135/183], Loss: 2.0820\n",
            "Epoch [3/30], Step [140/183], Loss: 4.4375\n",
            "Epoch [3/30], Step [145/183], Loss: 4.5078\n",
            "Epoch [3/30], Step [150/183], Loss: 6.2109\n",
            "Epoch [3/30], Step [155/183], Loss: 3.1523\n",
            "Epoch [3/30], Step [160/183], Loss: 2.5352\n",
            "Epoch [3/30], Step [165/183], Loss: 3.9375\n",
            "Epoch [3/30], Step [170/183], Loss: 3.9922\n",
            "Epoch [3/30], Step [175/183], Loss: 3.6152\n",
            "Epoch [3/30], Step [180/183], Loss: 4.1641\n",
            "Epoch [4/30], Step [5/183], Loss: 2.7383\n",
            "Epoch [4/30], Step [10/183], Loss: 2.4727\n",
            "Epoch [4/30], Step [15/183], Loss: 3.0469\n",
            "Epoch [4/30], Step [20/183], Loss: 8.9297\n",
            "Epoch [4/30], Step [25/183], Loss: 3.0566\n",
            "Epoch [4/30], Step [30/183], Loss: 3.1953\n",
            "Epoch [4/30], Step [35/183], Loss: 5.2227\n",
            "Epoch [4/30], Step [40/183], Loss: 2.6426\n",
            "Epoch [4/30], Step [45/183], Loss: 1.3164\n",
            "Epoch [4/30], Step [50/183], Loss: 2.7617\n",
            "Epoch [4/30], Step [55/183], Loss: 3.5039\n",
            "Epoch [4/30], Step [60/183], Loss: 1.6396\n",
            "Epoch [4/30], Step [65/183], Loss: 6.4375\n",
            "Epoch [4/30], Step [70/183], Loss: 2.7812\n",
            "Epoch [4/30], Step [75/183], Loss: 2.4922\n",
            "Epoch [4/30], Step [80/183], Loss: 2.1113\n",
            "Epoch [4/30], Step [85/183], Loss: 2.8047\n",
            "Epoch [4/30], Step [90/183], Loss: 3.0078\n",
            "Epoch [4/30], Step [95/183], Loss: 3.1504\n",
            "Epoch [4/30], Step [100/183], Loss: 3.4844\n",
            "Epoch [4/30], Step [105/183], Loss: 3.3008\n",
            "Epoch [4/30], Step [110/183], Loss: 3.4141\n",
            "Epoch [4/30], Step [115/183], Loss: 4.9297\n",
            "Epoch [4/30], Step [120/183], Loss: 5.3594\n",
            "Epoch [4/30], Step [125/183], Loss: 3.0625\n",
            "Epoch [4/30], Step [130/183], Loss: 6.2734\n",
            "Epoch [4/30], Step [135/183], Loss: 2.6328\n",
            "Epoch [4/30], Step [140/183], Loss: 2.5312\n",
            "Epoch [4/30], Step [145/183], Loss: 3.5566\n",
            "Epoch [4/30], Step [150/183], Loss: 2.9590\n",
            "Epoch [4/30], Step [155/183], Loss: 3.5918\n",
            "Epoch [4/30], Step [160/183], Loss: 4.3906\n",
            "Epoch [4/30], Step [165/183], Loss: 3.0312\n",
            "Epoch [4/30], Step [170/183], Loss: 3.8789\n",
            "Epoch [4/30], Step [175/183], Loss: 3.2832\n",
            "Epoch [4/30], Step [180/183], Loss: 3.7188\n",
            "Epoch [5/30], Step [5/183], Loss: 1.4941\n",
            "Epoch [5/30], Step [10/183], Loss: 2.4902\n",
            "Epoch [5/30], Step [15/183], Loss: 3.3027\n",
            "Epoch [5/30], Step [20/183], Loss: 8.6562\n",
            "Epoch [5/30], Step [25/183], Loss: 2.7207\n",
            "Epoch [5/30], Step [30/183], Loss: 3.0762\n",
            "Epoch [5/30], Step [35/183], Loss: 2.0547\n",
            "Epoch [5/30], Step [40/183], Loss: 2.8672\n",
            "Epoch [5/30], Step [45/183], Loss: 2.4629\n",
            "Epoch [5/30], Step [50/183], Loss: 4.7773\n",
            "Epoch [5/30], Step [55/183], Loss: 2.6680\n",
            "Epoch [5/30], Step [60/183], Loss: 6.8242\n",
            "Epoch [5/30], Step [65/183], Loss: 2.3164\n",
            "Epoch [5/30], Step [70/183], Loss: 6.2578\n",
            "Epoch [5/30], Step [75/183], Loss: 3.3828\n",
            "Epoch [5/30], Step [80/183], Loss: 2.4199\n",
            "Epoch [5/30], Step [85/183], Loss: 3.2480\n",
            "Epoch [5/30], Step [90/183], Loss: 4.7422\n",
            "Epoch [5/30], Step [95/183], Loss: 3.9316\n",
            "Epoch [5/30], Step [100/183], Loss: 6.3672\n",
            "Epoch [5/30], Step [105/183], Loss: 3.6992\n",
            "Epoch [5/30], Step [110/183], Loss: 3.7168\n",
            "Epoch [5/30], Step [115/183], Loss: 3.5273\n",
            "Epoch [5/30], Step [120/183], Loss: 3.4004\n",
            "Epoch [5/30], Step [125/183], Loss: 2.3398\n",
            "Epoch [5/30], Step [130/183], Loss: 4.6406\n",
            "Epoch [5/30], Step [135/183], Loss: 2.7148\n",
            "Epoch [5/30], Step [140/183], Loss: 5.5352\n",
            "Epoch [5/30], Step [145/183], Loss: 3.3340\n",
            "Epoch [5/30], Step [150/183], Loss: 2.9824\n",
            "Epoch [5/30], Step [155/183], Loss: 3.5293\n",
            "Epoch [5/30], Step [160/183], Loss: 3.0449\n",
            "Epoch [5/30], Step [165/183], Loss: 3.6484\n",
            "Epoch [5/30], Step [170/183], Loss: 2.0859\n",
            "Epoch [5/30], Step [175/183], Loss: 3.8613\n",
            "Epoch [5/30], Step [180/183], Loss: 2.8125\n",
            "Epoch [6/30], Step [5/183], Loss: 3.5938\n",
            "Epoch [6/30], Step [10/183], Loss: 3.0117\n",
            "Epoch [6/30], Step [15/183], Loss: 6.0742\n",
            "Epoch [6/30], Step [20/183], Loss: 4.4961\n",
            "Epoch [6/30], Step [25/183], Loss: 3.5879\n",
            "Epoch [6/30], Step [30/183], Loss: 3.0781\n",
            "Epoch [6/30], Step [35/183], Loss: 3.3535\n",
            "Epoch [6/30], Step [40/183], Loss: 3.1094\n",
            "Epoch [6/30], Step [45/183], Loss: 1.7852\n",
            "Epoch [6/30], Step [50/183], Loss: 2.5586\n",
            "Epoch [6/30], Step [55/183], Loss: 8.7891\n",
            "Epoch [6/30], Step [60/183], Loss: 4.4453\n",
            "Epoch [6/30], Step [65/183], Loss: 3.6484\n",
            "Epoch [6/30], Step [70/183], Loss: 6.3398\n",
            "Epoch [6/30], Step [75/183], Loss: 1.2607\n",
            "Epoch [6/30], Step [80/183], Loss: 2.7676\n",
            "Epoch [6/30], Step [85/183], Loss: 1.8975\n",
            "Epoch [6/30], Step [90/183], Loss: 5.3711\n",
            "Epoch [6/30], Step [95/183], Loss: 2.6387\n",
            "Epoch [6/30], Step [100/183], Loss: 3.3438\n",
            "Epoch [6/30], Step [105/183], Loss: 0.7930\n",
            "Epoch [6/30], Step [110/183], Loss: 2.1777\n",
            "Epoch [6/30], Step [115/183], Loss: 3.3242\n",
            "Epoch [6/30], Step [120/183], Loss: 2.8828\n",
            "Epoch [6/30], Step [125/183], Loss: 1.9453\n",
            "Epoch [6/30], Step [130/183], Loss: 4.1055\n",
            "Epoch [6/30], Step [135/183], Loss: 2.8027\n",
            "Epoch [6/30], Step [140/183], Loss: 4.0898\n",
            "Epoch [6/30], Step [145/183], Loss: 5.2852\n",
            "Epoch [6/30], Step [150/183], Loss: 3.6172\n",
            "Epoch [6/30], Step [155/183], Loss: 5.3359\n",
            "Epoch [6/30], Step [160/183], Loss: 3.7773\n",
            "Epoch [6/30], Step [165/183], Loss: 2.7402\n",
            "Epoch [6/30], Step [170/183], Loss: 3.0059\n",
            "Epoch [6/30], Step [175/183], Loss: 5.5664\n",
            "Epoch [6/30], Step [180/183], Loss: 4.4805\n",
            "Epoch [7/30], Step [5/183], Loss: 3.0840\n",
            "Epoch [7/30], Step [10/183], Loss: 3.3535\n",
            "Epoch [7/30], Step [15/183], Loss: 2.7285\n",
            "Epoch [7/30], Step [20/183], Loss: 3.0020\n",
            "Epoch [7/30], Step [25/183], Loss: 4.9922\n",
            "Epoch [7/30], Step [30/183], Loss: 2.3125\n",
            "Epoch [7/30], Step [35/183], Loss: 3.0410\n",
            "Epoch [7/30], Step [40/183], Loss: 1.3330\n",
            "Epoch [7/30], Step [45/183], Loss: 1.9199\n",
            "Epoch [7/30], Step [50/183], Loss: 2.1055\n",
            "Epoch [7/30], Step [55/183], Loss: 3.6836\n",
            "Epoch [7/30], Step [60/183], Loss: 5.2383\n",
            "Epoch [7/30], Step [65/183], Loss: 2.2656\n",
            "Epoch [7/30], Step [70/183], Loss: 3.0957\n",
            "Epoch [7/30], Step [75/183], Loss: 3.2285\n",
            "Epoch [7/30], Step [80/183], Loss: 2.2051\n",
            "Epoch [7/30], Step [85/183], Loss: 1.9824\n",
            "Epoch [7/30], Step [90/183], Loss: 3.6191\n",
            "Epoch [7/30], Step [95/183], Loss: 1.5684\n",
            "Epoch [7/30], Step [100/183], Loss: 8.9141\n",
            "Epoch [7/30], Step [105/183], Loss: 4.3789\n",
            "Epoch [7/30], Step [110/183], Loss: 4.2305\n",
            "Epoch [7/30], Step [115/183], Loss: 1.2939\n",
            "Epoch [7/30], Step [120/183], Loss: 3.7266\n",
            "Epoch [7/30], Step [125/183], Loss: 3.0781\n",
            "Epoch [7/30], Step [130/183], Loss: 5.2891\n",
            "Epoch [7/30], Step [135/183], Loss: 3.3184\n",
            "Epoch [7/30], Step [140/183], Loss: 4.0625\n",
            "Epoch [7/30], Step [145/183], Loss: 1.8623\n",
            "Epoch [7/30], Step [150/183], Loss: 1.3984\n",
            "Epoch [7/30], Step [155/183], Loss: 3.7520\n",
            "Epoch [7/30], Step [160/183], Loss: 4.1445\n",
            "Epoch [7/30], Step [165/183], Loss: 3.1211\n",
            "Epoch [7/30], Step [170/183], Loss: 3.2871\n",
            "Epoch [7/30], Step [175/183], Loss: 4.9570\n",
            "Epoch [7/30], Step [180/183], Loss: 3.5176\n",
            "Epoch [8/30], Step [5/183], Loss: 6.6172\n",
            "Epoch [8/30], Step [10/183], Loss: 4.5156\n",
            "Epoch [8/30], Step [15/183], Loss: 3.4082\n",
            "Epoch [8/30], Step [20/183], Loss: 2.1973\n",
            "Epoch [8/30], Step [25/183], Loss: 3.7578\n",
            "Epoch [8/30], Step [30/183], Loss: 3.0762\n",
            "Epoch [8/30], Step [35/183], Loss: 3.7852\n",
            "Epoch [8/30], Step [40/183], Loss: 3.8105\n",
            "Epoch [8/30], Step [45/183], Loss: 3.3105\n",
            "Epoch [8/30], Step [50/183], Loss: 3.7910\n",
            "Epoch [8/30], Step [55/183], Loss: 2.0527\n",
            "Epoch [8/30], Step [60/183], Loss: 2.0215\n",
            "Epoch [8/30], Step [65/183], Loss: 2.2207\n",
            "Epoch [8/30], Step [70/183], Loss: 3.6758\n",
            "Epoch [8/30], Step [75/183], Loss: 2.7031\n",
            "Epoch [8/30], Step [80/183], Loss: 2.6191\n",
            "Epoch [8/30], Step [85/183], Loss: 3.9688\n",
            "Epoch [8/30], Step [90/183], Loss: 2.5605\n",
            "Epoch [8/30], Step [95/183], Loss: 3.0137\n",
            "Epoch [8/30], Step [100/183], Loss: 4.1328\n",
            "Epoch [8/30], Step [105/183], Loss: 1.8701\n",
            "Epoch [8/30], Step [110/183], Loss: 2.7930\n",
            "Epoch [8/30], Step [115/183], Loss: 3.4336\n",
            "Epoch [8/30], Step [120/183], Loss: 1.3760\n",
            "Epoch [8/30], Step [125/183], Loss: 2.6582\n",
            "Epoch [8/30], Step [130/183], Loss: 3.2266\n",
            "Epoch [8/30], Step [135/183], Loss: 4.7539\n",
            "Epoch [8/30], Step [140/183], Loss: 6.3984\n",
            "Epoch [8/30], Step [145/183], Loss: 2.6523\n",
            "Epoch [8/30], Step [150/183], Loss: 2.2480\n",
            "Epoch [8/30], Step [155/183], Loss: 4.2148\n",
            "Epoch [8/30], Step [160/183], Loss: 4.5430\n",
            "Epoch [8/30], Step [165/183], Loss: 2.4629\n",
            "Epoch [8/30], Step [170/183], Loss: 3.3535\n",
            "Epoch [8/30], Step [175/183], Loss: 9.5859\n",
            "Epoch [8/30], Step [180/183], Loss: 3.2617\n",
            "Epoch [9/30], Step [5/183], Loss: 4.0586\n",
            "Epoch [9/30], Step [10/183], Loss: 5.9023\n",
            "Epoch [9/30], Step [15/183], Loss: 2.4492\n",
            "Epoch [9/30], Step [20/183], Loss: 2.8438\n",
            "Epoch [9/30], Step [25/183], Loss: 2.1289\n",
            "Epoch [9/30], Step [30/183], Loss: 4.9688\n",
            "Epoch [9/30], Step [35/183], Loss: 1.9844\n",
            "Epoch [9/30], Step [40/183], Loss: 3.3145\n",
            "Epoch [9/30], Step [45/183], Loss: 4.1680\n",
            "Epoch [9/30], Step [50/183], Loss: 3.8027\n",
            "Epoch [9/30], Step [55/183], Loss: 3.0312\n",
            "Epoch [9/30], Step [60/183], Loss: 2.0332\n",
            "Epoch [9/30], Step [65/183], Loss: 3.8809\n",
            "Epoch [9/30], Step [70/183], Loss: 2.2051\n",
            "Epoch [9/30], Step [75/183], Loss: 3.4219\n",
            "Epoch [9/30], Step [80/183], Loss: 5.7344\n",
            "Epoch [9/30], Step [85/183], Loss: 4.9883\n",
            "Epoch [9/30], Step [90/183], Loss: 2.7148\n",
            "Epoch [9/30], Step [95/183], Loss: 3.0938\n",
            "Epoch [9/30], Step [100/183], Loss: 3.7949\n",
            "Epoch [9/30], Step [105/183], Loss: 3.2441\n",
            "Epoch [9/30], Step [110/183], Loss: 3.2930\n",
            "Epoch [9/30], Step [115/183], Loss: 2.5703\n",
            "Epoch [9/30], Step [120/183], Loss: 6.3750\n",
            "Epoch [9/30], Step [125/183], Loss: 8.5781\n",
            "Epoch [9/30], Step [130/183], Loss: 2.7031\n",
            "Epoch [9/30], Step [135/183], Loss: 7.9258\n",
            "Epoch [9/30], Step [140/183], Loss: 3.2402\n",
            "Epoch [9/30], Step [145/183], Loss: 2.8672\n",
            "Epoch [9/30], Step [150/183], Loss: 3.7129\n",
            "Epoch [9/30], Step [155/183], Loss: 2.3633\n",
            "Epoch [9/30], Step [160/183], Loss: 2.4727\n",
            "Epoch [9/30], Step [165/183], Loss: 4.2031\n",
            "Epoch [9/30], Step [170/183], Loss: 3.5098\n",
            "Epoch [9/30], Step [175/183], Loss: 2.0098\n",
            "Epoch [9/30], Step [180/183], Loss: 1.5537\n",
            "Epoch [10/30], Step [5/183], Loss: 2.9551\n",
            "Epoch [10/30], Step [10/183], Loss: 2.9004\n",
            "Epoch [10/30], Step [15/183], Loss: 5.1992\n",
            "Epoch [10/30], Step [20/183], Loss: 3.3945\n",
            "Epoch [10/30], Step [25/183], Loss: 4.2891\n",
            "Epoch [10/30], Step [30/183], Loss: 2.6719\n",
            "Epoch [10/30], Step [35/183], Loss: 3.1348\n",
            "Epoch [10/30], Step [40/183], Loss: 1.2900\n",
            "Epoch [10/30], Step [45/183], Loss: 3.6328\n",
            "Epoch [10/30], Step [50/183], Loss: 6.1055\n",
            "Epoch [10/30], Step [55/183], Loss: 2.2656\n",
            "Epoch [10/30], Step [60/183], Loss: 5.5898\n",
            "Epoch [10/30], Step [65/183], Loss: 2.7969\n",
            "Epoch [10/30], Step [70/183], Loss: 3.4297\n",
            "Epoch [10/30], Step [75/183], Loss: 4.9531\n",
            "Epoch [10/30], Step [80/183], Loss: 2.5156\n",
            "Epoch [10/30], Step [85/183], Loss: 6.7266\n",
            "Epoch [10/30], Step [90/183], Loss: 2.4492\n",
            "Epoch [10/30], Step [95/183], Loss: 2.8965\n",
            "Epoch [10/30], Step [100/183], Loss: 4.4297\n",
            "Epoch [10/30], Step [105/183], Loss: 2.3672\n",
            "Epoch [10/30], Step [110/183], Loss: 2.6973\n",
            "Epoch [10/30], Step [115/183], Loss: 3.1582\n",
            "Epoch [10/30], Step [120/183], Loss: 3.2109\n",
            "Epoch [10/30], Step [125/183], Loss: 3.7559\n",
            "Epoch [10/30], Step [130/183], Loss: 3.8398\n",
            "Epoch [10/30], Step [135/183], Loss: 3.4961\n",
            "Epoch [10/30], Step [140/183], Loss: 2.4180\n",
            "Epoch [10/30], Step [145/183], Loss: 2.3672\n",
            "Epoch [10/30], Step [150/183], Loss: 3.7734\n",
            "Epoch [10/30], Step [155/183], Loss: 2.2754\n",
            "Epoch [10/30], Step [160/183], Loss: 2.9629\n",
            "Epoch [10/30], Step [165/183], Loss: 2.6543\n",
            "Epoch [10/30], Step [170/183], Loss: 5.3945\n",
            "Epoch [10/30], Step [175/183], Loss: 5.2891\n",
            "Epoch [10/30], Step [180/183], Loss: 2.6719\n",
            "Epoch [11/30], Step [5/183], Loss: 1.9424\n",
            "Epoch [11/30], Step [10/183], Loss: 1.6426\n",
            "Epoch [11/30], Step [15/183], Loss: 7.7305\n",
            "Epoch [11/30], Step [20/183], Loss: 2.4375\n",
            "Epoch [11/30], Step [25/183], Loss: 5.6406\n",
            "Epoch [11/30], Step [30/183], Loss: 3.1328\n",
            "Epoch [11/30], Step [35/183], Loss: 1.8496\n",
            "Epoch [11/30], Step [40/183], Loss: 8.6953\n",
            "Epoch [11/30], Step [45/183], Loss: 2.4844\n",
            "Epoch [11/30], Step [50/183], Loss: 3.0879\n",
            "Epoch [11/30], Step [55/183], Loss: 2.9980\n",
            "Epoch [11/30], Step [60/183], Loss: 4.3906\n",
            "Epoch [11/30], Step [65/183], Loss: 6.4609\n",
            "Epoch [11/30], Step [70/183], Loss: 3.4082\n",
            "Epoch [11/30], Step [75/183], Loss: 5.2578\n",
            "Epoch [11/30], Step [80/183], Loss: 4.9180\n",
            "Epoch [11/30], Step [85/183], Loss: 3.2109\n",
            "Epoch [11/30], Step [90/183], Loss: 2.8574\n",
            "Epoch [11/30], Step [95/183], Loss: 2.6660\n",
            "Epoch [11/30], Step [100/183], Loss: 2.3750\n",
            "Epoch [11/30], Step [105/183], Loss: 2.9590\n",
            "Epoch [11/30], Step [110/183], Loss: 3.4043\n",
            "Epoch [11/30], Step [115/183], Loss: 1.2842\n",
            "Epoch [11/30], Step [120/183], Loss: 3.1992\n",
            "Epoch [11/30], Step [125/183], Loss: 5.1328\n",
            "Epoch [11/30], Step [130/183], Loss: 5.7461\n",
            "Epoch [11/30], Step [135/183], Loss: 2.5410\n",
            "Epoch [11/30], Step [140/183], Loss: 3.6367\n",
            "Epoch [11/30], Step [145/183], Loss: 3.8867\n",
            "Epoch [11/30], Step [150/183], Loss: 5.3164\n",
            "Epoch [11/30], Step [155/183], Loss: 2.4531\n",
            "Epoch [11/30], Step [160/183], Loss: 3.2109\n",
            "Epoch [11/30], Step [165/183], Loss: 1.1367\n",
            "Epoch [11/30], Step [170/183], Loss: 3.6016\n",
            "Epoch [11/30], Step [175/183], Loss: 1.9863\n",
            "Epoch [11/30], Step [180/183], Loss: 2.1074\n",
            "Epoch [12/30], Step [5/183], Loss: 2.5312\n",
            "Epoch [12/30], Step [10/183], Loss: 6.8984\n",
            "Epoch [12/30], Step [15/183], Loss: 2.7441\n",
            "Epoch [12/30], Step [20/183], Loss: 4.7656\n",
            "Epoch [12/30], Step [25/183], Loss: 6.1992\n",
            "Epoch [12/30], Step [30/183], Loss: 5.4531\n",
            "Epoch [12/30], Step [35/183], Loss: 2.4883\n",
            "Epoch [12/30], Step [40/183], Loss: 2.8613\n",
            "Epoch [12/30], Step [45/183], Loss: 2.4043\n",
            "Epoch [12/30], Step [50/183], Loss: 1.1475\n",
            "Epoch [12/30], Step [55/183], Loss: 1.3877\n",
            "Epoch [12/30], Step [60/183], Loss: 5.0742\n",
            "Epoch [12/30], Step [65/183], Loss: 2.8574\n",
            "Epoch [12/30], Step [70/183], Loss: 3.4648\n",
            "Epoch [12/30], Step [75/183], Loss: 3.4824\n",
            "Epoch [12/30], Step [80/183], Loss: 3.1992\n",
            "Epoch [12/30], Step [85/183], Loss: 4.4297\n",
            "Epoch [12/30], Step [90/183], Loss: 3.2871\n",
            "Epoch [12/30], Step [95/183], Loss: 1.5723\n",
            "Epoch [12/30], Step [100/183], Loss: 6.9023\n",
            "Epoch [12/30], Step [105/183], Loss: 3.1230\n",
            "Epoch [12/30], Step [110/183], Loss: 2.0332\n",
            "Epoch [12/30], Step [115/183], Loss: 3.9805\n",
            "Epoch [12/30], Step [120/183], Loss: 1.5029\n",
            "Epoch [12/30], Step [125/183], Loss: 2.6484\n",
            "Epoch [12/30], Step [130/183], Loss: 2.0332\n",
            "Epoch [12/30], Step [135/183], Loss: 4.7852\n",
            "Epoch [12/30], Step [140/183], Loss: 3.2129\n",
            "Epoch [12/30], Step [145/183], Loss: 3.4395\n",
            "Epoch [12/30], Step [150/183], Loss: 4.0312\n",
            "Epoch [12/30], Step [155/183], Loss: 3.2715\n",
            "Epoch [12/30], Step [160/183], Loss: 2.5664\n",
            "Epoch [12/30], Step [165/183], Loss: 2.2070\n",
            "Epoch [12/30], Step [170/183], Loss: 2.8770\n",
            "Epoch [12/30], Step [175/183], Loss: 2.7988\n",
            "Epoch [12/30], Step [180/183], Loss: 3.1035\n",
            "Epoch [13/30], Step [5/183], Loss: 2.3027\n",
            "Epoch [13/30], Step [10/183], Loss: 1.9023\n",
            "Epoch [13/30], Step [15/183], Loss: 2.1328\n",
            "Epoch [13/30], Step [20/183], Loss: 4.2695\n",
            "Epoch [13/30], Step [25/183], Loss: 2.1309\n",
            "Epoch [13/30], Step [30/183], Loss: 6.1211\n",
            "Epoch [13/30], Step [35/183], Loss: 3.0488\n",
            "Epoch [13/30], Step [40/183], Loss: 3.1211\n",
            "Epoch [13/30], Step [45/183], Loss: 2.2910\n",
            "Epoch [13/30], Step [50/183], Loss: 3.3945\n",
            "Epoch [13/30], Step [55/183], Loss: 4.4570\n",
            "Epoch [13/30], Step [60/183], Loss: 3.8594\n",
            "Epoch [13/30], Step [65/183], Loss: 3.8691\n",
            "Epoch [13/30], Step [70/183], Loss: 3.3691\n",
            "Epoch [13/30], Step [75/183], Loss: 3.9785\n",
            "Epoch [13/30], Step [80/183], Loss: 2.7676\n",
            "Epoch [13/30], Step [85/183], Loss: 3.1660\n",
            "Epoch [13/30], Step [90/183], Loss: 5.1172\n",
            "Epoch [13/30], Step [95/183], Loss: 3.1504\n",
            "Epoch [13/30], Step [100/183], Loss: 4.7305\n",
            "Epoch [13/30], Step [105/183], Loss: 4.0508\n",
            "Epoch [13/30], Step [110/183], Loss: 4.8906\n",
            "Epoch [13/30], Step [115/183], Loss: 5.2695\n",
            "Epoch [13/30], Step [120/183], Loss: 2.2676\n",
            "Epoch [13/30], Step [125/183], Loss: 3.2070\n",
            "Epoch [13/30], Step [130/183], Loss: 2.6426\n",
            "Epoch [13/30], Step [135/183], Loss: 3.4082\n",
            "Epoch [13/30], Step [140/183], Loss: 3.9238\n",
            "Epoch [13/30], Step [145/183], Loss: 1.9570\n",
            "Epoch [13/30], Step [150/183], Loss: 1.2734\n",
            "Epoch [13/30], Step [155/183], Loss: 1.4150\n",
            "Epoch [13/30], Step [160/183], Loss: 2.0918\n",
            "Epoch [13/30], Step [165/183], Loss: 2.8262\n",
            "Epoch [13/30], Step [170/183], Loss: 4.8203\n",
            "Epoch [13/30], Step [175/183], Loss: 3.2324\n",
            "Epoch [13/30], Step [180/183], Loss: 3.0938\n",
            "Epoch [14/30], Step [5/183], Loss: 4.8320\n",
            "Epoch [14/30], Step [10/183], Loss: 5.3164\n",
            "Epoch [14/30], Step [15/183], Loss: 1.3193\n",
            "Epoch [14/30], Step [20/183], Loss: 5.1914\n",
            "Epoch [14/30], Step [25/183], Loss: 2.9453\n",
            "Epoch [14/30], Step [30/183], Loss: 6.6172\n",
            "Epoch [14/30], Step [35/183], Loss: 2.2773\n",
            "Epoch [14/30], Step [40/183], Loss: 3.8789\n",
            "Epoch [14/30], Step [45/183], Loss: 2.5723\n",
            "Epoch [14/30], Step [50/183], Loss: 3.0059\n",
            "Epoch [14/30], Step [55/183], Loss: 2.2012\n",
            "Epoch [14/30], Step [60/183], Loss: 1.8496\n",
            "Epoch [14/30], Step [65/183], Loss: 1.9863\n",
            "Epoch [14/30], Step [70/183], Loss: 4.9531\n",
            "Epoch [14/30], Step [75/183], Loss: 2.3809\n",
            "Epoch [14/30], Step [80/183], Loss: 2.4434\n",
            "Epoch [14/30], Step [85/183], Loss: 3.7344\n",
            "Epoch [14/30], Step [90/183], Loss: 2.5762\n",
            "Epoch [14/30], Step [95/183], Loss: 1.0742\n",
            "Epoch [14/30], Step [100/183], Loss: 3.2324\n",
            "Epoch [14/30], Step [105/183], Loss: 2.6914\n",
            "Epoch [14/30], Step [110/183], Loss: 2.8281\n",
            "Epoch [14/30], Step [115/183], Loss: 4.3945\n",
            "Epoch [14/30], Step [120/183], Loss: 3.1816\n",
            "Epoch [14/30], Step [125/183], Loss: 4.0781\n",
            "Epoch [14/30], Step [130/183], Loss: 1.9219\n",
            "Epoch [14/30], Step [135/183], Loss: 2.4297\n",
            "Epoch [14/30], Step [140/183], Loss: 2.4746\n",
            "Epoch [14/30], Step [145/183], Loss: 2.7871\n",
            "Epoch [14/30], Step [150/183], Loss: 1.4805\n",
            "Epoch [14/30], Step [155/183], Loss: 4.2852\n",
            "Epoch [14/30], Step [160/183], Loss: 7.8008\n",
            "Epoch [14/30], Step [165/183], Loss: 4.1758\n",
            "Epoch [14/30], Step [170/183], Loss: 1.8467\n",
            "Epoch [14/30], Step [175/183], Loss: 4.6406\n",
            "Epoch [14/30], Step [180/183], Loss: 4.7578\n",
            "Epoch [15/30], Step [5/183], Loss: 2.9258\n",
            "Epoch [15/30], Step [10/183], Loss: 4.3789\n",
            "Epoch [15/30], Step [15/183], Loss: 2.7109\n",
            "Epoch [15/30], Step [20/183], Loss: 1.8711\n",
            "Epoch [15/30], Step [25/183], Loss: 5.3047\n",
            "Epoch [15/30], Step [30/183], Loss: 3.9707\n",
            "Epoch [15/30], Step [35/183], Loss: 4.6719\n",
            "Epoch [15/30], Step [40/183], Loss: 3.8770\n",
            "Epoch [15/30], Step [45/183], Loss: 4.9688\n",
            "Epoch [15/30], Step [50/183], Loss: 1.9785\n",
            "Epoch [15/30], Step [55/183], Loss: 2.3789\n",
            "Epoch [15/30], Step [60/183], Loss: 2.4043\n",
            "Epoch [15/30], Step [65/183], Loss: 2.6426\n",
            "Epoch [15/30], Step [70/183], Loss: 2.5449\n",
            "Epoch [15/30], Step [75/183], Loss: 2.2617\n",
            "Epoch [15/30], Step [80/183], Loss: 4.7773\n",
            "Epoch [15/30], Step [85/183], Loss: 3.5566\n",
            "Epoch [15/30], Step [90/183], Loss: 3.3008\n",
            "Epoch [15/30], Step [95/183], Loss: 6.8359\n",
            "Epoch [15/30], Step [100/183], Loss: 6.4062\n",
            "Epoch [15/30], Step [105/183], Loss: 3.9863\n",
            "Epoch [15/30], Step [110/183], Loss: 3.7246\n",
            "Epoch [15/30], Step [115/183], Loss: 3.8184\n",
            "Epoch [15/30], Step [120/183], Loss: 3.5137\n",
            "Epoch [15/30], Step [125/183], Loss: 3.1758\n",
            "Epoch [15/30], Step [130/183], Loss: 3.4844\n",
            "Epoch [15/30], Step [135/183], Loss: 3.2305\n",
            "Epoch [15/30], Step [140/183], Loss: 3.6992\n",
            "Epoch [15/30], Step [145/183], Loss: 1.2822\n",
            "Epoch [15/30], Step [150/183], Loss: 2.2422\n",
            "Epoch [15/30], Step [155/183], Loss: 7.5977\n",
            "Epoch [15/30], Step [160/183], Loss: 2.7266\n",
            "Epoch [15/30], Step [165/183], Loss: 2.1992\n",
            "Epoch [15/30], Step [170/183], Loss: 3.7520\n",
            "Epoch [15/30], Step [175/183], Loss: 3.4648\n",
            "Epoch [15/30], Step [180/183], Loss: 5.5352\n",
            "Epoch [16/30], Step [5/183], Loss: 5.1680\n",
            "Epoch [16/30], Step [10/183], Loss: 4.6758\n",
            "Epoch [16/30], Step [15/183], Loss: 5.4297\n",
            "Epoch [16/30], Step [20/183], Loss: 6.5000\n",
            "Epoch [16/30], Step [25/183], Loss: 3.6055\n",
            "Epoch [16/30], Step [30/183], Loss: 2.6816\n",
            "Epoch [16/30], Step [35/183], Loss: 3.3906\n",
            "Epoch [16/30], Step [40/183], Loss: 2.6484\n",
            "Epoch [16/30], Step [45/183], Loss: 8.2578\n",
            "Epoch [16/30], Step [50/183], Loss: 5.2695\n",
            "Epoch [16/30], Step [55/183], Loss: 2.6543\n",
            "Epoch [16/30], Step [60/183], Loss: 2.7578\n",
            "Epoch [16/30], Step [65/183], Loss: 3.7930\n",
            "Epoch [16/30], Step [70/183], Loss: 4.0742\n",
            "Epoch [16/30], Step [75/183], Loss: 4.3477\n",
            "Epoch [16/30], Step [80/183], Loss: 2.0508\n",
            "Epoch [16/30], Step [85/183], Loss: 2.4238\n",
            "Epoch [16/30], Step [90/183], Loss: 2.4180\n",
            "Epoch [16/30], Step [95/183], Loss: 3.0820\n",
            "Epoch [16/30], Step [100/183], Loss: 3.7852\n",
            "Epoch [16/30], Step [105/183], Loss: 2.1895\n",
            "Epoch [16/30], Step [110/183], Loss: 3.7285\n",
            "Epoch [16/30], Step [115/183], Loss: 3.0625\n",
            "Epoch [16/30], Step [120/183], Loss: 2.9902\n",
            "Epoch [16/30], Step [125/183], Loss: 5.2227\n",
            "Epoch [16/30], Step [130/183], Loss: 5.1875\n",
            "Epoch [16/30], Step [135/183], Loss: 4.0156\n",
            "Epoch [16/30], Step [140/183], Loss: 3.1875\n",
            "Epoch [16/30], Step [145/183], Loss: 4.1953\n",
            "Epoch [16/30], Step [150/183], Loss: 1.6006\n",
            "Epoch [16/30], Step [155/183], Loss: 2.0312\n",
            "Epoch [16/30], Step [160/183], Loss: 2.6875\n",
            "Epoch [16/30], Step [165/183], Loss: 3.7090\n",
            "Epoch [16/30], Step [170/183], Loss: 3.5293\n",
            "Epoch [16/30], Step [175/183], Loss: 2.6895\n",
            "Epoch [16/30], Step [180/183], Loss: 3.4121\n",
            "Epoch [17/30], Step [5/183], Loss: 3.5605\n",
            "Epoch [17/30], Step [10/183], Loss: 1.7520\n",
            "Epoch [17/30], Step [15/183], Loss: 1.6377\n",
            "Epoch [17/30], Step [20/183], Loss: 3.7031\n",
            "Epoch [17/30], Step [25/183], Loss: 7.7930\n",
            "Epoch [17/30], Step [30/183], Loss: 6.8867\n",
            "Epoch [17/30], Step [35/183], Loss: 3.3184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnMgutvCzJfZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdKmBQCuzJmx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC-V32IgzJpV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVaUenu0zJrv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}